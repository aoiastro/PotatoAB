# PotatoAB - iOS向け AI卓上ロボット

PotatoAB（ポテト・エービー）は、SwiftPMを用いて構築されたiOSデバイス向けの完全ローカル動作AI卓上ロボットアプリです。内部でMLXを活用し、Qwen3-1.7B（4ビット量子化）モデルをローカルで実行します。

## 機能
- **完全ローカル推論**: `mlx-swift`を利用し、初回起動時にQwenモデルをデバイス上にダウンロードして実行します。以降はインターネット接続が不要です。
- **ウェイクワード検出**: Appleのネイティブの音声認識(`SFSpeechRecognizer`)を使用し、日本語で「ポテト」と呼びかけると反応します。
- **動的な表情と読み上げ機能 (TTS)**: LLMが生成した応答に合わせて画面上の表情（Happy、Sad、Thinkingなど）が変化し、日本語の合成音声(`AVSpeechSynthesizer`)で応答を読み上げます。
- **JSON駆動**: LLMのシステムプロンプトによって厳密にJSON出力を強制しており、音声と表情の連動を完璧に行います。

## 必要要件
- iOS 17.0以降のデバイス（LLMのパフォーマンス上、A14 Bionic以降を推奨）
- Xcode 15以降（ローカル開発用）

## AltStore経由でのインストール（サイドロード）
GitHub Actionsのワークフロー (`.github/workflows/build.yml`) が組み込まれており、`main`ブランチへコミットすると自動で署名なし(unsigned)の `.ipa` ファイルがビルドされます。

1. このプロジェクトをご自身のGitHubリポジトリにプッシュします。
2. GitHubリポジトリの **Actions** タブを開きます。
3. 最新の成功したワークフローから、ビルド成果物である `PotatoAB_unsigned.ipa` をダウンロードします。
4. その `.ipa` ファイルをiOSデバイスへ転送します。
5. **AltStore** (または SideStore / Sideloadly 等) を使用して、デバイスにインストールします。

## 使い方
1. iOSデバイスでPotatoABを開きます。初回起動時はモデルのダウンロード(約1〜1.5GB)に少し時間がかかります。
2. 初回起動時に「マイク」と「音声認識」の許可を求められるので、許可してください。
3. デバイスを横向き（ランドスケープ）に置いてマウントします。
4. 「ポテト」と呼びかけます。画面の顔文字が耳のマーク(👂)に変わり、聞き取りを開始します。
5. 質問やお願いを話しかけます。(例：「今日の気分はどうですか？」)
6. 考え中(🤔)の表情になったあと、ローカルのMLX Qwenモデルの推論が完了すると、表情を変えて声であなたに返答します。
